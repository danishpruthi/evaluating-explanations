## Evaluating Explanations

This is the code-repository corresponding the following paper:

> [Evaluating Explanations: How much do explanations from the teacher aid students?](https://arxiv.org/pdf/2012.00893.pdf)
> 
> *Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen*
>
> Transactions of the Association for Computational Linguistics (TACL, 2021)


**Update**: If you are looking to evaluate explanations using our metric, you can use the [**this repository**](https://github.com/CoderPat/learning-scaffold) instead as this: (1) computes our proposed simulatability metric _and also_ generates explanations that are optimized according to our metric, and (2) overall is more structured and maintained. 
